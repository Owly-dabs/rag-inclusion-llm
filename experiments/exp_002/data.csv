url,raw_content,summary,topic,code,extra,answer,notes
https://github.com/fastai/fastai/pull/3948,"fix torch.jit.script on TimmBody   

Creating a model based on a timm backbone and then converting it to TorchScript currently fails.  import torch from fastai.vision.learner import create_timm_model  model = create_timm_model('resnet18', 1) torch.jit.script(model)  RuntimeError:  Could not cast value of type Tuple[int, int] to bool:   File ""/tmp/ipykernel_114724/2853927905.py"", line 8     def forward(self,x): return self.model.forward_features(x) if self.needs_pool else self.model(x)                                                                   ~~~~~~~~~~~~~~~ <--- HERE  TorchScript can't implicitly convert the tuple into a boolean. So we should do it explicitly instead with is not None. ","1. General Description    This PR fixes torch.jit.script compatibility for TimmBody models in fastai. Currently, converting a timm-based model to TorchScript fails due to a type casting issue.  2. Reason for the change    TorchScript cannot implicitly convert a Tuple\[int, int] to bool, leading to a RuntimeError when scripting timm-based models. This fix is needed for TorchScript export workflows.  3. Description of the change    The PR changes the forward logic in TimmBody to explicitly check is not None instead of relying on implicit tuple-to-bool conversion, allowing timm-based models to be scripted without errors. ","Syntax - Internal API modularity, readability - Code Quality, Syntax, and Formatting","class TimmBody(nn.Module):     def __init__(self, model, pretrained:bool=True, cut=None, n_in:int=3):         super().__init__()
self.needs_pool = model.default_cfg.get('pool_size', None)
        self.model = model if cut is None else cut_model(model, cut)      def forward(self,x): return self.model.forward_features(x) if self.needs_pool else self.model(x)","def forward(self,x): return self.model.forward_features(x) if self.needs_pool else self.model(x)
def create_timm_model(arch, n_out, cut=None, pretrained=True, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,                      concat_pool=True, pool=True, lin_ftrs=None, ps=0.5, first_bn=True, bn_final=False, lin_first=False, y_range=None, **kwargs):     ""Create custom architecture using `arch`, `n_in` and `n_out` from the `timm` library""     model = timm.create_model(arch, pretrained=pretrained, num_classes=0, in_chans=n_in, **kwargs)     body = TimmBody(model, pretrained, None, n_in)     nf = body.model.num_features     res = add_head(body, nf, n_out, init=init, head=custom_head, concat_pool=concat_pool, pool=body.needs_pool,","class TimmBody(nn.Module):     def __init__(self, model, pretrained:bool=True, cut=None, n_in:int=3):         super().__init__()        self.needs_pool = model.default_cfg.get('pool_size', None) is not None         self.model = model if cut is None else cut_model(model, cut)      def forward(self,x): return self.model.forward_features(x) if self.needs_pool else self.model(x)",
https://github.com/fastai/fastai/pull/4098,"  Update plot lr find This PR addresses a long-standing oversight in the plot_lr_find method of the Recorder class. Specifically, the method accepts a return_fig parameter, but the fig object was never actually returned—making the parameter redundant in practice.  I discovered and initially suggested a fix for this issue in the fast.ai forums over 18 months ago. However, it appears the update was never incorporated into the codebase.  What’s Changed:  At the end of the plot_lr_find function, I’ve added:  if return_fig: return fig  Why This Matters:  This change ensures the fig object is returned when return_fig=True, enabling users to further manipulate or save the figure outside the method. This is particularly useful when plotting with suggestions and needing to customize or export the plot.  Without this fix, users might assume that setting return_fig=True returns the figure, but it does not—leading to confusion and unnecessary debugging.  Fix Summary:      Respects the intent of the return_fig flag     Enables access to the matplotlib figure object     Aligns function behavior with user expectations and documentation  Let me know if further tests or documentation updates are needed. ","##General Description This PR fixes the plot_lr_find method in the Recorder class, which has a return_fig parameter that was never functioning as intended because the fig object was not returned, rendering the parameter ineffective.  ##Reason for the change The change is needed to align the function’s behavior with user expectations and documentation, allowing users to retrieve the matplotlib figure for further manipulation or saving when return_fig=True, and to eliminate confusion and unnecessary debugging caused by the parameter not working.  ##Description of the change At the end of the plot_lr_find function, the line if return_fig: return fig has been added to return the fig object when return_fig=True, ensuring the parameter now functions correctly.",Visualization - Plot - Data Visualization and Analytics,"      for (val, idx), nm, color in zip(suggestions, nms, colors):             ax.plot(val, idx, 'o', label=nm, c=color)         ax.legend(loc='best') mk_class(""SuggestionMethod"", **{o.__name__.capitalize():o for o in [valley,slide,minimum,steep]},          doc=""All possible suggestion methods as convience attributes to get tab-completion and typo-proofing"")","def plot_lr_find(self:Recorder, skip_end=5, return_fig=True, suggestions=None, nms=None, **kwargs):     ""Plot the result of an LR Finder test (won't work if you didn't do `learn.lr_find()` before)""     lrs    = self.lrs    if skip_end==0 else self.lrs   [:-skip_end]     losses = self.losses if skip_end==0 else self.losses[:-skip_end]     fig, ax = plt.subplots(1,1)     ax.plot(lrs, losses)     ax.set_ylabel(""Loss"")     ax.set_xlabel(""Learning Rate"")     ax.set_xscale('log')     if suggestions:         colors = plt.rcParams['axes.prop_cycle'].by_key()['color'][1:]         for (val, idx), nm, color in zip(suggestions, nms, colors):             ax.plot(val, idx, 'o', label=nm, c=color)         ax.legend(loc='best')","for (val, idx), nm, color in zip(suggestions, nms, colors):             ax.plot(val, idx, 'o', label=nm, c=color)         ax.legend(loc='best')     if return_fig: fig  # %% ../../nbs/14_callback.schedule.ipynb 89 mk_class(""SuggestionMethod"", **{o.__name__.capitalize():o for o in [valley,slide,minimum,steep]},",
https://github.com/fastai/fastai/pull/4073,"Fix #4072 untar_data() ignores provided base
Describe the bug The untar_data() function takes an optional base parameter whose purpose is to specify the ""Directory containing config file and base of relative paths"". However, the path is ignored and the default fastai_cfg() location is always used.  It looks like the untar_data() function always passes fastai_cfg() and base to FastDownload. The result is that FastDownload always uses fastai_cfg() and never creates a new config based upon the provided base. FastDownload only uses base to create a new config if no config is provided.  To Reproduce Steps to reproduce the behavior:      Call untar_data() with a custom base: untar_data(URLs.PETS, base='/tmp/my_location')  Expected behavior Data is downloaded and untarred to /tmp/my_location. Actually, it gets download to the default ~/.fastai directory.  Additional context  A workaround is to set FASTAI_HOME envar to change the base, but this doesn't work well to programmatically change the base. Alternatively, don't use untar_data() and use FastDownload directly:  # This doesn't use /tmp/my_location as the base, it still uses ~/.fastai # path = untar_data(URLs.PETS, base='/tmp/my_location')  # WORKAROUND: Use FastDownload directly. Pass cfg=None to FastDownload instead of fastai_cfg() # so that FastDownload creates a new config based on the provided base. d = FastDownload(cfg=None, module=fastai.data, base='/tmp/my_location') path = d.get(URLs.PETS, extract_key='data')
Updated untar_data() to pass a None config to FastDownload when an overridden base is provided. FastDownload always uses a provided config object and ignores base if both are given.
","##General Description The issue reports that the untar_data() function in fastai ignores the provided base directory parameter and always uses the default ~/.fastai directory for downloads. This behavior occurs because untar_data() always passes fastai_cfg() to FastDownload along with the base, causing FastDownload to ignore the base parameter. The expected behavior is for data to be downloaded and extracted into the user-specified base directory when provided.  ##Reason for the change Users need the ability to programmatically control the download and extraction location by specifying the base parameter in untar_data(). The current behavior forces all downloads into the default fastai config path, limiting flexibility and breaking expected parameter usage. This change is needed to align the function behavior with user expectations and documentation.  ##Description of the change The change updates untar_data() to pass cfg=None to FastDownload when a user-specified base parameter is provided. This allows FastDownload to create a new configuration based on the provided base directory instead of using the default fastai configuration path. As a result, data will correctly download and extract into the specified base location when using untar_data().",Data Collection - Data Collection Management and Web Scraping,"    data:Path=None, # Optional override for `Config`'s `data` key     c_key:str='data', # Key in `Config` where to extract file     force_download:bool=False, # Setting to `True` will overwrite any existing copy of data     base:str='~/.fastai' # Directory containing config file and base of relative paths
) -> Path: # Path to extracted file(s)     ""Download `url` using `FastDownload.get`""     d = FastDownload(fastai_cfg(), module=fastai.data, archive=archive, data=data, base=base)
   return d.get(url, force=force_download, extract_key=c_key)","def untar_data(     url:str, # File to download     archive:Path=None, # Optional override for `Config`'s `archive` key     data:Path=None, # Optional override for `Config`'s `data` key     c_key:str='data', # Key in `Config` where to extract file     force_download:bool=False, # Setting to `True` will overwrite any existing copy of data     base:str='~/.fastai' # Directory containing config file and base of relative paths
) -> Path: # Path to extracted file(s)     ""Download `url` using `FastDownload.get`""     d = FastDownload(fastai_cfg(), module=fastai.data, archive=archive, data=data, base=base)
    return d.get(url, force=force_download, extract_key=c_key)","data:Path=None, # Optional override for `Config`'s `data` key     c_key:str='data', # Key in `Config` where to extract file     force_download:bool=False, # Setting to `True` will overwrite any existing copy of data    base:str=None # Directory containing config file and base of relative paths ) -> Path: # Path to extracted file(s)     ""Download `url` using `FastDownload.get`""    cfg = None     if base is None:         cfg = fastai_cfg()         # A base must be provided as FastDownload initializes a Path with it even         # though the config provided is ultimately used instead.         base = '~/.fastai'     d = FastDownload(cfg, module=fastai.data, archive=archive, data=data, base=base)     return d.get(url, force=force_download, extract_key=c_key)",
https://github.com/fastai/fastai/pull/3884,"The self.types loop in infer_idx hits the break on line 404 one element earlier than the commit prior to #3872 and the idx gets set to one value lower. This means that the rm_tfms variable within contains the create method of PILBase when it previously didn't.  Making the create method of PILBase method safely accept a PILImage means this rm_tfms variable that gets set to the test_tl.tfms.fs attribute is executed successfully when compose_tfms is called.  Whilst I think its good that a PILImage can be created from another PILImage, this fix doesn't preserve the original behavior of how infer_idx previously functioned. Not sure if this is a bigger problem or if change this is sufficient
I think this simple fix is the correct fix, as modifying infer_idx and related code to perform a strict type check would probably cause more issues in the future.","##General Description The self.types loop in the infer_idx function now stops one element earlier than before PR #3872, causing the idx value to be lower. This results in the rm_tfms variable containing PILBase's create method, which previously did not occur. Since PILBase.create now accepts a PILImage, execution during compose_tfms calls continues without errors.  ##Reason for the change The current change unintentionally alters infer_idx behavior, affecting type inference and transform application. It is unclear if this deeper change will lead to broader issues, and enforcing strict type checks in infer_idx may introduce further complications.  ##Description of the change The proposed change retains the current implementation where PILBase.create accepts a PILImage, allowing successful execution with the adjusted idx behavior. It suggests accepting this fix without modifying infer_idx or enforcing stricter type checks to maintain stability and forward compatibility.",Application - Environment Setup Validation,"        if isinstance(fn,Tensor): fn = fn.numpy()         if isinstance(fn,ndarray): return cls(Image.fromarray(fn))         if isinstance(fn,bytes): fn = io.BytesIO(fn)         if isinstance(fn,Image.Image) and not isinstance(fn,cls): return cls(fn)         return cls(load_image(fn, **merge(cls._open_args, kwargs)))      def show(self, ctx=None, **kwargs):","class PILBase(Image.Image, metaclass=BypassNewMeta):     ""Base class for a Pillow `Image` that can show itself and convert to a Tensor""     _bypass_type=Image.Image     _show_args = {'cmap':'viridis'}     _open_args = {'mode': 'RGB'}     @classmethod     def create(cls, fn:Path|str|Tensor|ndarray|bytes|Image.Image, **kwargs):         ""Return an Image from `fn`""         if isinstance(fn,TensorImage): fn = fn.permute(1,2,0).type(torch.uint8)         if isinstance(fn,TensorMask): fn = fn.type(torch.uint8)         if isinstance(fn,Tensor): fn = fn.numpy()         if isinstance(fn,ndarray): return cls(Image.fromarray(fn))         if isinstance(fn,bytes): fn = io.BytesIO(fn)         if isinstance(fn,Image.Image) and not isinstance(fn,cls): return cls(fn)
return cls(load_image(fn, **merge(cls._open_args, kwargs))) 

 def test_set(       dsets:Datasets|TfmdLists, # Map- or iterable-style dataset from which to load the data       test_items, # Items in test dataset       rm_tfms=None, # Start index of `Transform`(s) from validation set in `dsets` to apply       with_labels:bool=False # Whether the test items contain labels   ):       ""Create a test set from `test_items` using validation transforms of `dsets`""       if isinstance(dsets, Datasets):           tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp]           test_tls = [tl._new(test_items, split_idx=1) for tl in tls]           if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(test_items)) for tl in test_tls]           else:               rm_tfms = tuplify(rm_tfms, match=test_tls)           for i,j in enumerate(rm_tfms): test_tls[i].tfms.fs = test_tls[i].tfms.fs[j:]           return Datasets(tls=test_tls)       elif isinstance(dsets, TfmdLists):           test_tl = dsets._new(test_items, split_idx=1)           if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(test_items))           test_tl.tfms.fs = test_tl.tfms.fs[rm_tfms:]           return test_tl       else: raise Exception(f""This method requires using the fastai library to assemble your data. Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}"") 

","if isinstance(fn,Tensor): fn = fn.numpy()         if isinstance(fn,ndarray): return cls(Image.fromarray(fn))         if isinstance(fn,bytes): fn = io.BytesIO(fn)        if isinstance(fn,Image.Image): return cls(fn)         return cls(load_image(fn, **merge(cls._open_args, kwargs)))      def show(self, ctx=None, **kwargs):",
https://github.com/fastai/fastai/pull/3593,"AttributeError: 'Figure' object has no property 'add_vert'
Describe the bug show_results is throwing an error after PR #3590  To Reproduce Steps to reproduce the behavior:      Run notebook 21_vision.learner     After the show_results are defined, try to use them (Working on minimal code example now)  Expected behavior Shouldn't get an error  Error with full stack trace
File /usr/local/lib/python3.8/dist-packages/matplotlib/artist.py:1064, in Artist.update(self, props)    1062             func = getattr(self, f""set_{k}"", None)    1063             if not callable(func): -> 1064                 raise AttributeError(f""{type(self).__name__!r} object ""    1065                                      f""has no property {k!r}"")    1066             ret.append(func(v))    1067 if ret:  AttributeError: 'Figure' object has no property 'add_vert'
Additional context There are no tests or usages when show__results are created so I think it would be good to add something minimal that tests those functions so it would fail in cases like this. As far as I can tell, the fix for this is to just remove add_vert=1 in all of the get_grid functions. Not sure if any other notebooks use that as well, but they would have the same issue if so

","##General Description The user reports an AttributeError in which a 'Figure' object has no property 'add_vert' when using show_results after PR #3590. This error occurs when running notebook 21_vision.learner and attempting to display results with show_results. The user is preparing a minimal reproducible example to clarify the issue.  ##Reason for the change The issue breaks expected usage of show_results, which should display results without error. Currently, there are no tests to catch this type of failure in the show_results path, allowing regressions to slip in unnoticed. It is necessary to address this to ensure notebook workflows do not fail unexpectedly for users.  ##Description of the change Remove the add_vert=1 argument from all get_grid function calls, which is causing the 'Figure' object attribute error. Investigate if any other notebooks use this pattern and correct them similarly to prevent further breakages. Add minimal tests for show_results to catch this type of regression in the future.",Quality - Deprecation - Code Quality Syntax and Formatting,"def show_batch(x:TensorImage, y:TensorImage, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):     if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, add_vert=1, figsize=figsize, double=True)
    for i in range(2):         ctxs[i::2] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs[i::2],range(max_n))]     return ctxs","def get_grid(n, nrows=None, ncols=None, figsize=None, double=False, title=None, return_fig=False,              flatten=True, **kwargs):     ""Return a grid of `n` axes, `rows` by `cols`""     if nrows:         ncols = ncols or int(np.ceil(n/nrows))     elif ncols:         nrows = nrows or int(np.ceil(n/ncols))     else:         nrows = int(math.sqrt(n))         ncols = int(np.ceil(n/nrows))     if double: ncols*=2 ; n*=2     fig,axs = subplots(nrows, ncols, figsize=figsize, **kwargs)     if flatten: axs = [ax if i<n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]     if title is not None: fig.suptitle(title, weight='bold', size=14)     return (fig,axs) if return_fig else axs","def show_batch(x:TensorImage, y:TensorImage, samples, ctxs=None, max_n=10, nrows=None, ncols=None, figsize=None, **kwargs):    if ctxs is None: ctxs = get_grid(min(len(samples), max_n), nrows=nrows, ncols=ncols, figsize=figsize, double=True)     for i in range(2):         ctxs[i::2] = [b.show(ctx=c, **kwargs) for b,c,_ in zip(samples.itemgot(i),ctxs[i::2],range(max_n))]     return ctxs",
https://github.com/fastai/fastai/pull/3643,"[Bugfix] Fix smoothloss on distributed 
Currently smoothloss behaves differently than all the other metric calculators, as it does not gather all the tensors when calculating. As a result, if using fastai distributed the reported training loss is actually wrong, as it's only shown for worker 0.","\##General Description The PR addresses a bug where smoothloss behaves differently from other metric calculators during distributed training. Currently, smoothloss does not gather tensors across all workers when calculating the loss. This results in incorrect reporting of training loss, showing only the value from worker 0.  \##Reason for the change The inconsistency causes incorrect loss values during distributed training, misleading users about the actual training state. Aligning smoothloss behavior with other metrics is necessary for accurate monitoring and consistency within fastai's distributed workflow.  \##Description of the change Modify smoothloss to gather tensors from all workers during distributed training. This ensures that the reported training loss correctly reflects the aggregated values across all workers, matching the behavior of other metric calculators in the library. ",ML - Dataprocessing Performance,"   def accumulate(self, learn):         self.count += 1         self.val = torch.lerp(to_detach(learn.loss.mean(), gather=False), self.val, self.beta)
   @property     def value(self): return self.val/(1-self.beta**self.count)","class AvgSmoothLoss(Metric):     ""Smooth average of the losses (exponentially weighted with `beta`)""     def __init__(self, beta=0.98): self.beta = beta     def reset(self):               self.count,self.val = 0,tensor(0.)     def accumulate(self, learn):         self.count += 1         self.val = torch.lerp(to_detach(learn.loss.mean(), gather=False), self.val, self.beta)
    @property     def value(self): return self.val/(1-self.beta**self.count)  
def to_detach(self,b,cpu=True,gather=True):         return self.dl.to_detach(b,cpu,gather) if hasattr(getattr(self,'dl',None),'to_detach') else to_detach(b,cpu,gather)","def accumulate(self, learn):         self.count += 1        self.val = torch.lerp(to_detach(learn.loss.mean()), self.val, self.beta)     @property     def value(self): return self.val/(1-self.beta**self.count)",
https://github.com/fastai/fastai/pull/3621,"Fix AccumMetric name.setter
This PR allows AccumMetric's name property to be set.","\##General Description This PR fixes an issue where the name property of AccumMetric could not be set. The lack of a functional setter prevented users from renaming metrics when needed. This change ensures that AccumMetric can have its name adjusted dynamically.  \##Reason for the change Allowing the name property to be set is important for flexibility in metric tracking and reporting. Without this, users cannot customize or correct metric names, which can cause confusion in logging and UI displays during training workflows.  \##Description of the change Implement a setter for the name property within the AccumMetric class. This enables users to assign or modify the metric's name as needed for clearer tracking and reporting in their training pipelines. ",Software Development - Software Development and Version Control,"    def name(self):  return self._name      @name.setter     def name(self, value): self._name = name"," Cell class AccumMetric(Metric):     ""Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.""     def __init__(self, func, dim_argmax=None, activation=ActivationType.No, thresh=None, to_np=False,                  invert_arg=False, flatten=True, name=None, **kwargs):         store_attr('func,dim_argmax,activation,thresh,flatten')         self._name = ifnone(name, self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__)         self.to_np,self.invert_args,self.kwargs = to_np,invert_arg,kwargs     def reset(self):         ""Clear all targs and preds""         self.targs,self.preds = [],[]     def accumulate(self, learn):         ""Store targs and preds from `learn`, using activation function and argmax as appropriate""         pred = learn.pred         if self.activation in [ActivationType.Softmax, ActivationType.BinarySoftmax]:             pred = F.softmax(pred, dim=self.dim_argmax)             if self.activation == ActivationType.BinarySoftmax: pred = pred[:, -1]         elif self.activation == ActivationType.Sigmoid: pred = torch.sigmoid(pred)         elif self.dim_argmax: pred = pred.argmax(dim=self.dim_argmax)         if self.thresh:  pred = (pred >= self.thresh)         self.accum_values(pred,learn.y,learn)     def accum_values(self, preds, targs,learn=None):         ""Store targs and preds""         to_d = learn.to_detach if learn is not None else to_detach         preds,targs = to_d(preds),to_d(targs)         if self.flatten: preds,targs = flatten_check(preds,targs)         self.preds.append(preds)         self.targs.append(targs)     def __call__(self, preds, targs):         ""Calculate metric on one batch of data""         self.reset()         self.accum_values(preds,targs)         return self.value     @property     def value(self):         ""Value of the metric using accumulated preds and targs""         if len(self.preds) == 0: return         preds,targs = torch.cat(self.preds),torch.cat(self.targs)         if self.to_np: preds,targs = preds.numpy(),targs.numpy()         return self.func(targs, preds, **self.kwargs) if self.invert_args else self.func(preds, targs, **self.kwargs)     @property     def name(self):  return self._name      @name.setter     def name(self, value): self._name = name","def name(self):  return self._name      @name.setter   def name(self, value): self._name = value  # Cell def skm_to_fastai(func, is_class=True, thresh=None, axis=-1, activation=None, **kwargs):",
https://github.com/fastai/fastai/pull/3606,"Fix pin_memory=True breaking (batch) Transforms
breaks: pin_memory=True makes DL return a list instead of a tuple and Transforms (e.g. batch_tfms) expect batches to be tuples.
The reason for the bug is that PyTorch's pin_memory (v1.10.0, v1.11.0) converts tuples into lists - so we need to convert back to a tuple.","##General Description This PR fixes an issue where enabling pin_memory=True causes DataLoader to return a list instead of a tuple. This breaks batch transforms that expect batches to be tuples. The problem occurs due to PyTorch converting tuples to lists when pin_memory is used.  ##Reason for the change Batch transforms fail when batches are provided as lists instead of tuples, interrupting data processing pipelines during training. Ensuring batch type consistency is necessary for transforms to function correctly with pin_memory enabled.  ##Description of the change Convert the list returned by DataLoader with pin_memory=True back into a tuple before applying batch transforms. This preserves compatibility with existing transforms while maintaining the benefits of pin_memory in the data loading pipeline.",ML - Dataprocessing correctness - Machine Learning Data Handling and Formats,"      self.before_iter()         self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses)         for b in _loaders[self.fake_l.num_workers==0](self.fake_l):
            if self.device is not None: b = to_device(b, self.device)             yield self.after_batch(b)         self.after_iter()
","class DataLoader(GetAttr):     _noop_methods = 'wif before_iter after_item before_batch after_batch after_iter'.split()     for o in _noop_methods: exec(f""def {o}(self, x=None, *args, **kwargs): return x"")     _methods = _noop_methods + 'create_batches create_item create_batch retain \         get_idxs sample shuffle_fn do_batch create_batch'.split()     _default = 'dataset'     def __init__(self, dataset=None, bs=None, num_workers=0, pin_memory=False, timeout=0, batch_size=None,                  shuffle=False, drop_last=False, indexed=None, n=None, device=None, persistent_workers=False, **kwargs):         if batch_size is not None: bs = batch_size # PyTorch compatibility         assert not (bs is None and drop_last)         if indexed is None: indexed = (hasattr(dataset,'__getitem__')                                        and not isinstance(dataset, IterableDataset))         if not indexed and shuffle: raise ValueError(""Can only shuffle an indexed dataset (not an iterable one)."")         if n is None:             try: n = len(dataset)             except TypeError: pass         store_attr('dataset,bs,shuffle,drop_last,indexed,n,pin_memory,timeout,device')         self.rng,self.num_workers,self.offs = random.Random(random.randint(0,2**32-1)),1,0         if sys.platform == ""win32"" and IN_NOTEBOOK and num_workers > 0:             print(""Due to IPython and Windows limitation, python multiprocessing isn't available now."")             print(""So `number_workers` is changed to 0 to avoid getting stuck"")             num_workers = 0         self.fake_l = _FakeLoader(self, pin_memory, num_workers, timeout, persistent_workers=persistent_workers)     def __len__(self):         if self.n is None: raise TypeError         if self.bs is None: return self.n         return self.n//self.bs + (0 if self.drop_last or self.n%self.bs==0 else 1)     def get_idxs(self):         idxs = Inf.count if self.indexed else Inf.nones         if self.n is not None: idxs = list(itertools.islice(idxs, self.n))         if self.shuffle: idxs = self.shuffle_fn(idxs)         return idxs     def sample(self):         return (b for i,b in enumerate(self.__idxs) if i//(self.bs or 1)%self.num_workers==self.offs)     def __iter__(self):         self.randomize()         self.before_iter()         self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses)         for b in _loaders[self.fake_l.num_workers==0](self.fake_l):            if self.device is not None: b = to_device(b, self.device)             yield self.after_batch(b)         self.after_iter()","self.before_iter()         self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses)         for b in _loaders[self.fake_l.num_workers==0](self.fake_l):             # pin_memory causes tuples to be converted to lists, so convert them back to tuples             if self.pin_memory and type(b) == list: b = tuple(b)             if self.device is not None: b = to_device(b, self.device)             yield self.after_batch(b)         self.after_iter()",
https://github.com/fastai/fastai/pull/3582,"Fix bug in URLs.path() in 04_data.external
In URLs.path(), there is a check to see if the file to download exists in folders data or models in URLs.LOCAL_PATH.  The key to the Config is set to models instead of model when forming the local path. The key should be one of ['archive', 'data', 'model', 'storage'] according to URLs docs","\##General Description This PR fixes a bug in the URLs.path() method used in 04\_data.external. The method checks if a file exists in local folders before downloading, but incorrectly uses the key 'models' instead of 'model' when forming the local path. This causes the path resolution to fail due to a mismatch with the expected config keys.  \##Reason for the change The incorrect key prevents proper detection of existing local files, forcing unnecessary downloads or raising errors. Aligning the key with the documented options ensures the method correctly checks local paths and behaves as intended.  \##Description of the change Replace the incorrect 'models' key with the correct 'model' key when forming the local path in URLs.path(). This allows proper file existence checks in local folders, matching the documented and expected behavior of the URLs utility. ",Data Collection - Data Collection Management and Web Scraping,"    def path(url='.', c_key='archive'):         ""Return local path where to download based on `c_key`""         fname = url.split('/')[-1]         local_path = URLs.LOCAL_PATH/('models' if c_key=='models' else 'data')/fname        if local_path.exists(): return local_path         return fastai_path(c_key)/fname","class URLs():     ""Global constants for dataset and model URLs.""     LOCAL_PATH = Path.cwd()     MDL = 'http://files.fast.ai/models/'     GOOGLE = 'https://storage.googleapis.com/'     S3  = 'https://s3.amazonaws.com/fast-ai-'     URL = f'{S3}sample/'    S3_IMAGE    = f'{S3}imageclas/'     S3_IMAGELOC = f'{S3}imagelocal/'     S3_AUDI     = f'{S3}audio/'     S3_NLP      = f'{S3}nlp/'     S3_COCO     = f'{S3}coco/'     S3_MODEL    = f'{S3}modelzoo/'     # main datasets     ADULT_SAMPLE        = f'{URL}adult_sample.tgz'     BIWI_SAMPLE         = f'{URL}biwi_sample.tgz'     CIFAR               = f'{URL}cifar10.tgz'     COCO_SAMPLE         = f'{S3_COCO}coco_sample.tgz'     COCO_TINY           = f'{S3_COCO}coco_tiny.tgz'     HUMAN_NUMBERS       = f'{URL}human_numbers.tgz'     IMDB                = f'{S3_NLP}imdb.tgz'     IMDB_SAMPLE         = f'{URL}imdb_sample.tgz'     ML_SAMPLE           = f'{URL}movie_lens_sample.tgz'     ML_100k             = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'     MNIST_SAMPLE        = f'{URL}mnist_sample.tgz'     MNIST_TINY          = f'{URL}mnist_tiny.tgz'     MNIST_VAR_SIZE_TINY = f'{S3_IMAGE}mnist_var_size_tiny.tgz'     PLANET_SAMPLE       = f'{URL}planet_sample.tgz'     PLANET_TINY         = f'{URL}planet_tiny.tgz'     IMAGENETTE          = f'{S3_IMAGE}imagenette2.tgz'     IMAGENETTE_160      = f'{S3_IMAGE}imagenette2-160.tgz'     IMAGENETTE_320      = f'{S3_IMAGE}imagenette2-320.tgz'     IMAGEWOOF           = f'{S3_IMAGE}imagewoof2.tgz'     IMAGEWOOF_160       = f'{S3_IMAGE}imagewoof2-160.tgz'     IMAGEWOOF_320       = f'{S3_IMAGE}imagewoof2-320.tgz'     IMAGEWANG           = f'{S3_IMAGE}imagewang.tgz'     IMAGEWANG_160       = f'{S3_IMAGE}imagewang-160.tgz'     IMAGEWANG_320       = f'{S3_IMAGE}imagewang-320.tgz'     # kaggle competitions download dogs-vs-cats -p {DOGS.absolute()}     DOGS = f'{URL}dogscats.tgz'     # image classification datasets     CALTECH_101  = f'{S3_IMAGE}caltech_101.tgz'     CARS         = f'{S3_IMAGE}stanford-cars.tgz'     CIFAR_100    = f'{S3_IMAGE}cifar100.tgz'     CUB_200_2011 = f'{S3_IMAGE}CUB_200_2011.tgz'     FLOWERS      = f'{S3_IMAGE}oxford-102-flowers.tgz'     FOOD         = f'{S3_IMAGE}food-101.tgz'     MNIST        = f'{S3_IMAGE}mnist_png.tgz'     PETS         = f'{S3_IMAGE}oxford-iiit-pet.tgz'     # NLP datasets     AG_NEWS                 = f'{S3_NLP}ag_news_csv.tgz'     AMAZON_REVIEWS          = f'{S3_NLP}amazon_review_full_csv.tgz'     AMAZON_REVIEWS_POLARITY = f'{S3_NLP}amazon_review_polarity_csv.tgz'     DBPEDIA                 = f'{S3_NLP}dbpedia_csv.tgz'     MT_ENG_FRA              = f'{S3_NLP}giga-fren.tgz'     SOGOU_NEWS              = f'{S3_NLP}sogou_news_csv.tgz'     WIKITEXT                = f'{S3_NLP}wikitext-103.tgz'     WIKITEXT_TINY           = f'{S3_NLP}wikitext-2.tgz'     YAHOO_ANSWERS           = f'{S3_NLP}yahoo_answers_csv.tgz'     YELP_REVIEWS            = f'{S3_NLP}yelp_review_full_csv.tgz'     YELP_REVIEWS_POLARITY   = f'{S3_NLP}yelp_review_polarity_csv.tgz'     # Image localization datasets     BIWI_HEAD_POSE     = f""{S3_IMAGELOC}biwi_head_pose.tgz""     CAMVID             = f'{S3_IMAGELOC}camvid.tgz'     CAMVID_TINY        = f'{URL}camvid_tiny.tgz'     LSUN_BEDROOMS      = f'{S3_IMAGE}bedroom.tgz'     PASCAL_2007        = f'{S3_IMAGELOC}pascal_2007.tgz'     PASCAL_2012        = f'{S3_IMAGELOC}pascal_2012.tgz'     # Audio classification datasets     MACAQUES           = f'{GOOGLE}ml-animal-sounds-datasets/macaques.zip'     ZEBRA_FINCH        = f'{GOOGLE}ml-animal-sounds-datasets/zebra_finch.zip'     # Medical Imaging datasets     #SKIN_LESION        = f'{S3_IMAGELOC}skin_lesion.tgz'     SIIM_SMALL         = f'{S3_IMAGELOC}siim_small.tgz'     TCGA_SMALL         = f'{S3_IMAGELOC}tcga_small.tgz'     #Pretrained models     OPENAI_TRANSFORMER = f'{S3_MODEL}transformer.tgz'     WT103_FWD          = f'{S3_MODEL}wt103-fwd.tgz'     WT103_BWD          = f'{S3_MODEL}wt103-bwd.tgz'     def path(url='.', c_key='archive'):         ""Return local path where to download based on `c_key`""         fname = url.split('/')[-1]         local_path = URLs.LOCAL_PATH/('models' if c_key=='models' else 'data')/fname        if local_path.exists(): return local_path         return fastai_path(c_key)/fname","def path(url='.', c_key='archive'):         ""Return local path where to download based on `c_key`""         fname = url.split('/')[-1]        local_path = URLs.LOCAL_PATH/('models' if c_key=='model' else 'data')/fname         if local_path.exists(): return local_path         return fastai_path(c_key)/fname",a good use case for including docs -- PR references docs 
https://github.com/fastai/fastai/pull/3583,"Making loss_not_reduced work with DiceLoss
When using loss_not_reduced context manager with DiceLoss, it would still reduce the loss by sum because it is using if self.reduction == 'mean' else logic.  The original issue I came across was the following:  learn = unet_learner(dls, resnet34, loss_func=DiceLoss(axis=1)) interp = SegmentationInterpretation.from_learner(learn)  The above code will result in:    1049     1050         with _C.DisableTorchFunction(): -> 1051             ret = func(*args, **kwargs)    1052             if func in get_default_nowrap_functions():    1053                 return ret  RuntimeError: shape '[8, -1]' is invalid for input of size 1","\##General Description This PR fixes DiceLoss not respecting the loss\_not\_reduced context manager. DiceLoss currently reduces the loss by summing even when reduction should be disabled, leading to shape mismatches during interpretation workflows. This specifically causes errors when using SegmentationInterpretation with DiceLoss.  \##Reason for the change Without respecting loss\_not\_reduced, DiceLoss returns a scalar instead of retaining per-element losses, breaking downstream operations that expect unreduced shapes. This prevents correct interpretation and analysis during segmentation model workflows.  \##Description of the change Update DiceLoss to respect the loss\_not\_reduced context by skipping reduction when this context is active. This ensures DiceLoss outputs unreduced tensors when required, preventing shape mismatch errors during segmentation interpretation workflows. ",ML - Metrics Loss - Machine Learning Model Evaluation Types and Techniques,"         union = (torch.sum(pred**2+targ, dim=sum_dims) if self.square_in_union             else torch.sum(pred+targ, dim=sum_dims))         dice_score = (2. * inter + self.smooth)/(union + self.smooth)         return ((1-dice_score).flatten().mean() if self.reduction == ""mean""             else (1-dice_score).flatten().sum())    @staticmethod     def _one_hot(x, classes, axis=1):         ""Creates one binay mask per class""","# Cell class DiceLoss:     ""Dice loss for segmentation""     def __init__(self, axis=1, smooth=1e-6, reduction=""sum"", square_in_union=False):         store_attr()     def __call__(self, pred, targ):         targ = self._one_hot(targ, pred.shape[self.axis])         pred, targ = TensorBase(pred), TensorBase(targ)         assert pred.shape == targ.shape, 'input and target dimensions differ, DiceLoss expects non one-hot targs'         pred = self.activation(pred)         sum_dims = list(range(2, len(pred.shape)))         inter = torch.sum(pred*targ, dim=sum_dims)         union = (torch.sum(pred**2+targ, dim=sum_dims) if self.square_in_union             else torch.sum(pred+targ, dim=sum_dims))         dice_score = (2. * inter + self.smooth)/(union + self.smooth)         return ((1-dice_score).flatten().mean() if self.reduction == ""mean""             else (1-dice_score).flatten().sum())    @staticmethod     def _one_hot(x, classes, axis=1):         ""Creates one binay mask per class""        return torch.stack([torch.where(x==c, 1, 0) for c in range(classes)], axis=axis)     def activation(self, x): return F.softmax(x, dim=self.axis)     def decodes(self, x):    return x.argmax(dim=self.axis)","union = (torch.sum(pred**2+targ, dim=sum_dims) if self.square_in_union             else torch.sum(pred+targ, dim=sum_dims))         dice_score = (2. * inter + self.smooth)/(union + self.smooth)        loss = 1- dice_score         if self.reduction == 'mean':             loss = loss.mean()         elif self.reduction == 'sum':             loss = loss.sum()         return loss    @staticmethod     def _one_hot(x, classes, axis=1):         ""Creates one binay mask per class""",
