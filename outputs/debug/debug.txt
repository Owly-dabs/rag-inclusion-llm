(CodeRegion(filename='.buildkite/Dockerfile.gpu', code='FROM nvidia/cuda:11.2.0-cudnn8-devel-ubuntu20.04\n\nARG REMOTE_CACHE_URL\nARG BUILDKITE_PULL_REQUEST\nARG BUILDKITE_COMMIT\nARG BUILDKITE_PULL_REQUEST_BASE_BRANCH\nARG PYTHON=3.7', explanation=None), CodeRegion(filename='.buildkite/Dockerfile.gpu', code='FROM nvidia/cuda:11.2.0-cudnn8-devel-ubuntu18.04\n\nARG REMOTE_CACHE_URL\nARG BUILDKITE_PULL_REQUEST\nARG BUILDKITE_COMMIT\nARG BUILDKITE_PULL_REQUEST_BASE_BRANCH\nARG PYTHON=3.7', explanation=None))
(CodeRegion(filename='.github/CODEOWNERS', code='# Dependencies\n/python/setup.py @richardliaw @ericl @edoakes\n\n# Formatting tool\n/ci/travis/format.sh @richardliaw @ericl @edoakes\n\n# Python worker.\n#/python/ray/ @ray-project/ray-core-python\n#!/python/ray/tune/ @ray-project/ray-core-python\n#!/python/ray/rllib/ @ray-project/ray-core-python\n\n# Java worker.', explanation=None), CodeRegion(filename='.github/CODEOWNERS', code='# Dependencies\n/python/setup.py @richardliaw @ericl @edoakes\n\n# Formatting tool\n/ci/travis/format.sh @richardliaw @ericl @edoakes\n\n# Docker image build script.\n/ci/travis/build-docker-images.py @amogkam @krfricke\n\n# Python worker.\n#/python/ray/ @ray-project/ray-core-python\n#!/python/ray/tune/ @ray-project/ray-core-python\n#!/python/ray/rllib/ @ray-project/ray-core-python\n\n# Java worker.', explanation=None))
(CodeRegion(filename='ci/travis/build-docker-images.py', code='    "cu110": "CUDA 11.0",\n    "cu102": "CUDA 10.2",\n    "cu101": "CUDA 10.1",\n}\n\n# The CUDA version to use for the ML Docker image.\nML_CUDA_VERSION = "cu112"\n\nDEFAULT_PYTHON_VERSION = "py37"\n\nIMAGE_NAMES = list(DOCKER_HUB_DESCRIPTION.keys())\n', explanation=None), CodeRegion(filename='ci/travis/build-docker-images.py', code='    "cu110": "CUDA 11.0",\n    "cu102": "CUDA 10.2",\n    "cu101": "CUDA 10.1",\n}\n\n# The CUDA version to use for the ML Docker image.\n# If changing the CUDA version in the below line, you should also change the base Docker\n# image being used in ~/.buildkite/Dockerfile.gpu to match the same image being used\n# here.\nML_CUDA_VERSION = "cu112"\n\nDEFAULT_PYTHON_VERSION = "py37"\n\nIMAGE_NAMES = list(DOCKER_HUB_DESCRIPTION.keys())\n', explanation=None))
(CodeRegion(filename='dashboard/client/package.json', code='  },\n  "devDependencies": {\n    "eslint-plugin-import": "2.20.1",\n    "eslint-plugin-prefer-arrow": "1.1.7",\n    "prettier": "2.3.0"\n  },\n  "scripts": {\n    "start": "react-scripts start",\n    "build": "react-scripts build",\n    "test": "react-scripts test",\n    "eject": "react-scripts eject",\n    "lint": "npm run eslint && npm run prettier",', explanation=None), CodeRegion(filename='dashboard/client/package.json', code='  },\n  "devDependencies": {\n    "eslint-plugin-import": "2.20.1",\n    "eslint-plugin-prefer-arrow": "1.1.7",\n    "prettier": "2.3.0"\n  },\n  "resolutions": {\n    "@types/react": "16.9.26"\n  },\n  "scripts": {\n    "start": "react-scripts start",\n    "build": "react-scripts build",\n    "test": "react-scripts test",\n    "eject": "react-scripts eject",\n    "lint": "npm run eslint && npm run prettier",', explanation=None))
(CodeRegion(filename='doc/source/data/dataset.rst', code='    :card:\n\n    Getting Started\n    ^^^\n\n    Start with our quick start tutorials for :ref:`working with Datasets<ray_datasets_quick_start>`\n    and :ref:`Dataset Pipelines<dataset_pipelines_quick_start>`.\n    These concrete examples will give you an idea of how to use Ray Datasets.\n\n    +++\n    .. link-button:: datasets_getting_started\n        :type: ref\n        :text: Get Started with Ray Datasets', explanation=None), CodeRegion(filename='doc/source/data/dataset.rst', code='    :card:\n\n    Getting Started\n    ^^^\n\n    Start with our quick start tutorials for :ref:`working with Datasets<ray_datasets_quick_start>`\n    and :ref:`Dataset Pipelines<pipelining_datasets>`.\n    These concrete examples will give you an idea of how to use Ray Datasets.\n\n    +++\n    .. link-button:: datasets_getting_started\n        :type: ref\n        :text: Get Started with Ray Datasets', explanation=None))
(CodeRegion(filename='doc/source/data/getting-started.rst', code='    shards = ds.split(n=16, locality_hints=workers)\n    # -> [Dataset(num_blocks=13, num_rows=650, schema=<class \'int\'>),\n    #     Dataset(num_blocks=13, num_rows=650, schema=<class \'int\'>), ...]\n\n    ray.get([w.train.remote(s) for w, s in zip(workers, shards)])\n    # -> [650, 650, ...]\n\n\n.. _dataset_pipelines_quick_start:\n\n-----------------------------\nDataset Pipelines Quick Start\n-----------------------------\n\nCreating a DatasetPipeline\n==========================\n\nA `DatasetPipeline <package-ref.html#datasetpipeline-api>`__ can be constructed in two ways: either by pipelining the execution of an existing Dataset (via ``Dataset.window``), or generating repeats of an existing Dataset (via ``Dataset.repeat``). Similar to Datasets, you can freely pass DatasetPipelines between Ray tasks, actors, and libraries. Get started with this synthetic data example:\n\n.. code-block:: python\n\n    import ray\n\n    def func1(i: int) -> int:\n        return i + 1\n\n    def func2(i: int) -> int:\n        return i * 2\n\n    def func3(i: int) -> int:\n        return i % 3\n\n    # Create a dataset and then create a pipeline from it.\n    base = ray.data.range(1000000)\n    print(base)\n    # -> Dataset(num_blocks=200, num_rows=1000000, schema=<class \'int\'>)\n    pipe = base.window(blocks_per_window=10)\n    print(pipe)\n    # -> DatasetPipeline(num_windows=20, num_stages=1)\n\n    # Applying transforms to pipelines adds more pipeline stages.\n    pipe = pipe.map(func1)\n    pipe = pipe.map(func2)\n    pipe = pipe.map(func3)\n    print(pipe)\n    # -> DatasetPipeline(num_windows=20, num_stages=4)\n\n    # Output can be pulled from the pipeline concurrently with its execution.\n    num_rows = 0\n    for row in pipe.iter_rows():\n        num_rows += 1\n    # ->\n    # Stage 0:  55%|█████████████████████████                |11/20 [00:02<00:00,  9.86it/s]\n    # Stage 1:  50%|██████████████████████                   |10/20 [00:02<00:01,  9.45it/s]\n    # Stage 2:  45%|███████████████████                      | 9/20 [00:02<00:01,  8.27it/s]\n    # Stage 3:  35%|████████████████                         | 8/20 [00:02<00:02,  5.33it/s]\n    print("Total num rows", num_rows)\n    # -> Total num rows 1000000\n\nYou can also create a DatasetPipeline from a custom iterator over dataset creators using ``DatasetPipeline.from_iterable``. For example, this is how you would implement ``Dataset.repeat`` and ``Dataset.window`` using ``from_iterable``:\n\n.. code-block:: python\n\n    import ray\n    from ray.data.dataset_pipeline import DatasetPipeline\n\n    # Equivalent to ray.data.range(1000).repeat(times=4)\n    source = ray.data.range(1000)\n    pipe = DatasetPipeline.from_iterable(\n        [lambda: source, lambda: source, lambda: source, lambda: source])\n\n    # Equivalent to ray.data.range(1000).window(blocks_per_window=10)\n    splits = ray.data.range(1000, parallelism=200).split(20)\n    pipe = DatasetPipeline.from_iterable([lambda s=s: s for s in splits])\n\n\nPer-Window Transformations\n==========================\n\nWhile most Dataset operations are per-row (e.g., map, filter), some operations apply to the Dataset as a whole (e.g., sort, shuffle). When applied to a pipeline, holistic transforms like shuffle are applied separately to each window in the pipeline:\n\n.. code-block:: python\n\n    # Example of randomly shuffling each window of a pipeline.\n    ray.data.from_items([0, 1, 2, 3, 4]) \\\n        .repeat(2) \\\n        .random_shuffle_each_window() \\\n        .show_windows()\n    # ->\n    # ----- Epoch 0 ------\n    # === Window 0 ===\n    # 4\n    # 3\n    # 1\n    # 0\n    # 2\n    # ----- Epoch 1 ------\n    # === Window 1 ===\n    # 2\n    # 1\n    # 4\n    # 0\n    # 3\n\nYou can also apply arbitrary transformations to each window using ``DatasetPipeline.foreach_window()``:\n\n.. code-block:: python\n\n    # Equivalent transformation using .foreach_window()\n    ray.data.from_items([0, 1, 2, 3, 4]) \\\n        .repeat(2) \\\n        .foreach_window(lambda w: w.random_shuffle()) \\\n        .show_windows()\n    # ->\n    # ----- Epoch 0 ------\n    # === Window 0 ===\n    # 1\n    # 0\n    # 4\n    # 2\n    # 3\n    # ----- Epoch 1 ------\n    # === Window 1 ===\n    # 4\n    # 2\n    # 0\n    # 3\n    # 1', explanation=None), CodeRegion(filename='doc/source/data/getting-started.rst', code="    shards = ds.split(n=16, locality_hints=workers)\n    # -> [Dataset(num_blocks=13, num_rows=650, schema=<class 'int'>),\n    #     Dataset(num_blocks=13, num_rows=650, schema=<class 'int'>), ...]\n\n    ray.get([w.train.remote(s) for w, s in zip(workers, shards)])\n    # -> [650, 650, ...]", explanation=None))
(CodeRegion(filename='doc/source/data/pipelining-compute.rst', code='\nPer-Window Transformations\n==========================\n\nWhile most Dataset operations are per-row (e.g., map, filter), some operations apply to the Dataset as a whole (e.g., sort, shuffle). When applied to a pipeline, holistic transforms like shuffle are applied separately to each window in the pipeline:\n\n.. code-block:: python\n\n    # Example of randomly shuffling each window of a pipeline.\n    ray.data.from_items([0, 1, 2, 3, 4]) \\\n        .repeat(2) \\\n        .random_shuffle_each_window() \\', explanation=None), CodeRegion(filename='doc/source/data/pipelining-compute.rst', code='\nPer-Window Transformations\n==========================\n\nWhile most Dataset operations are per-row (e.g., map, filter), some operations apply to the Dataset as a whole (e.g., sort, shuffle). When applied to a pipeline, holistic transforms like shuffle are applied separately to each window in the pipeline:\n\n.. important::\n\n   Windowed shuffle or global shuffle are expensive operations. Use only if you really need them.\n   Alternatively, you may consider local shuffle after converting to_tf() or to_torch(), if simple shuffle is sufficient.\n\n.. code-block:: python\n\n    # Example of randomly shuffling each window of a pipeline.\n    ray.data.from_items([0, 1, 2, 3, 4]) \\\n        .repeat(2) \\\n        .random_shuffle_each_window() \\', explanation=None))
(CodeRegion(filename='python/ray/util/accelerators/__init__.py', code='from ray.util.accelerators.accelerators import (\n    NVIDIA_TESLA_V100,\n    NVIDIA_TESLA_P100,\n    NVIDIA_TESLA_T4,\n    NVIDIA_TESLA_P4,\n    NVIDIA_TESLA_K80,\n)\n\n__all__ = [\n    "NVIDIA_TESLA_V100",\n    "NVIDIA_TESLA_P100",\n    "NVIDIA_TESLA_T4",', explanation=None), CodeRegion(filename='python/ray/util/accelerators/__init__.py', code='from ray.util.accelerators.accelerators import (\n    NVIDIA_TESLA_V100,\n    NVIDIA_TESLA_P100,\n    NVIDIA_TESLA_T4,\n    NVIDIA_TESLA_P4,\n    NVIDIA_TESLA_K80,\n    NVIDIA_TESLA_A100,\n)\n\n__all__ = [\n    "NVIDIA_TESLA_V100",\n    "NVIDIA_TESLA_P100",\n    "NVIDIA_TESLA_T4",', explanation=None))
(CodeRegion(filename='python/ray/util/accelerators/__init__.py', code='__all__ = [\n    "NVIDIA_TESLA_V100",\n    "NVIDIA_TESLA_P100",\n    "NVIDIA_TESLA_T4",\n    "NVIDIA_TESLA_P4",\n    "NVIDIA_TESLA_K80",\n]', explanation=None), CodeRegion(filename='python/ray/util/accelerators/__init__.py', code='__all__ = [\n    "NVIDIA_TESLA_V100",\n    "NVIDIA_TESLA_P100",\n    "NVIDIA_TESLA_T4",\n    "NVIDIA_TESLA_P4",\n    "NVIDIA_TESLA_K80",\n    "NVIDIA_TESLA_A100",\n]', explanation=None))
(CodeRegion(filename='rllib/BUILD', code='    data = glob(["examples/serving/*.py"]),\n)\n\nsh_test(\n    name = "env/tests/test_remote_inference_cartpole_lstm",\n    tags = ["team:ml", "env"],\n    size = "medium",\n    srcs = ["env/tests/test_policy_client_server_setup.sh"],\n    args = ["remote", "cartpole_lstm"],\n    data = glob(["examples/serving/*.py"]),\n)\n\nsh_test(', explanation=None), CodeRegion(filename='rllib/BUILD', code='    data = glob(["examples/serving/*.py"]),\n)\n\nsh_test(\n    name = "env/tests/test_remote_inference_cartpole_lstm",\n    tags = ["team:ml", "env"],\n    size = "large",\n    srcs = ["env/tests/test_policy_client_server_setup.sh"],\n    args = ["remote", "cartpole_lstm"],\n    data = glob(["examples/serving/*.py"]),\n)\n\nsh_test(', explanation=None))
(CodeRegion(filename='rllib/models/tf/misc.py', code='\ntf1, tf, tfv = try_import_tf()\n\n\ndef normc_initializer(std: float = 1.0) -> Any:\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(np.float32)\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n\n    return _initializer\n\n', explanation=None), CodeRegion(filename='rllib/models/tf/misc.py', code='\ntf1, tf, tfv = try_import_tf()\n\n\ndef normc_initializer(std: float = 1.0) -> Any:\n    def _initializer(shape, dtype=None, partition_info=None):\n        out = np.random.randn(*shape).astype(\n            dtype.name if hasattr(dtype, "name") else dtype or np.float32\n        )\n        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))\n        return tf.constant(out)\n\n    return _initializer\n\n', explanation=None))
(CodeRegion(filename='.buildkite/Dockerfile.gpu', code='FROM nvidia/cuda:11.2.0-cudnn8-devel-ubuntu18.04\n\nARG REMOTE_CACHE_URL\nARG BUILDKITE_PULL_REQUEST\nARG BUILDKITE_COMMIT\nARG BUILDKITE_PULL_REQUEST_BASE_BRANCH\nARG PYTHON=3.7', explanation=None), CodeRegion(filename='.buildkite/Dockerfile.gpu', code='FROM nvidia/cuda:11.2.0-cudnn8-devel-ubuntu20.04\n\nARG REMOTE_CACHE_URL\nARG BUILDKITE_PULL_REQUEST\nARG BUILDKITE_COMMIT\nARG BUILDKITE_PULL_REQUEST_BASE_BRANCH\nARG PYTHON=3.7', explanation=None))
